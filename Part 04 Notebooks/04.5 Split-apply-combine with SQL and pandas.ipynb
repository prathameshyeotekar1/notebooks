{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split-apply-combine - with SQL and _pandas_\n",
    "The split-apply-combine processing pattern is also sometimes referred to as a data grouping operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split-apply-combine processing segments a dataset (split), applies some processing to each segment (apply) and then brings the results back into a single results set (combine).\n",
    "\n",
    "When handling data this allows different processing to be applied to different subsets of the data, depending on the properties of the subset, but still to treat the data as a single whole collection.\n",
    "\n",
    "Both SQL and _pandas_ realise the split-apply-combine pattern using  a mechanism based on 'grouping', which segments a dataset by supplying an attribute list whose values determine the membership of each subset.  \n",
    "\n",
    "We've seen SQL and _pandas_ statements that SELECT rows of data on some condition - say `CourseCode ='TM351'`. SELECTing will extract those rows from the dataset; grouping keeps those rows as a subset within the complete dataset.\n",
    "\n",
    "In this Notebook we're going to look briefly at the SQL GROUPBY and HAVING clauses and then consider the _pandas_ `groupby()` method over DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split-apply-combine in SQL\n",
    "\n",
    "### The GROUP BY clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SQL GROUP BY clause takes a list of column names and, using the values in those columns, produces an internal form of the table with data grouped into subgroups.  There is one subgroup for each unique combination of values from those rows.\n",
    "\n",
    "There is a limitation in SQL that once a GROUP BY has been applied the result returned can have, at most, one row per group.\n",
    "\n",
    "This requires that any SELECT statement therefore contains combinations of:\n",
    "- the columns that appeared in the GROUP BY\n",
    "- aggregate functions over the groups\n",
    "- constant values.\n",
    "\n",
    "We'll use _pandasql_ for our SQL examples, but this should extend to other SQL implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Start by importing the sqldf function from pandasql:\n",
    "from pandasql import sqldf\n",
    "\n",
    "# Then create a simple wrapper function to allow us to supply the query 'q' without \n",
    "# the surrounding syntax.\n",
    "pysqldf = lambda q: sqldf(q, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Our sample dataset is in the CSV file 'mixed_module_data' in the data folder\n",
    "mixed_module_data = pd.read_csv('data/mixed_module_data.csv')\n",
    "mixed_module_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# We can apply the GROUP BY to our table, and using the select ModuleCode we can see the values\n",
    "#  of ModuleCode that segment the data. There's a group for each of these values.\n",
    "query = '''\n",
    "SELECT ModuleCode as 'These are the groups in ModuleCode' \n",
    "FROM mixed_module_data \n",
    "GROUP BY ModuleCode; \n",
    "'''\n",
    "result = pysqldf(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUP BY and aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# We know what happens if we apply an aggregate function to the whole table.\n",
    "query = ''' SELECT COUNT(*) as 'how many rows' FROM mixed_module_data; '''\n",
    "result = pysqldf(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# The GROUP BY segments the whole table into groups, \n",
    "# and the aggregate function is applied to each group.\n",
    "query = ''' \n",
    "SELECT COUNT(*) as 'how many rows' \n",
    "FROM mixed_module_data \n",
    "GROUP BY ModuleCode; \n",
    "'''\n",
    "result = pysqldf(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Maybe with some more useful labelling we can see what is happening.\n",
    "query = ''' \n",
    "SELECT ModuleCode, COUNT(*) as 'how many student/marks, grouped by module' \n",
    "FROM mixed_module_data \n",
    "GROUP BY ModuleCode; '''\n",
    "result = pysqldf(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "### Exercise\n",
    "What do you think will happen if we group by `ModuleCode` and `Year`? \n",
    "What does this grouping represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Here's the SQL code to achieve this, \n",
    "# but before you run the cell - what do you think will happen?\n",
    "\n",
    "query = ''' \n",
    "SELECT ModuleCode, Year, COUNT(*) as 'how many student/marks' \n",
    "FROM mixed_module_data \n",
    "GROUP BY ModuleCode, Year; '''\n",
    "result = pysqldf(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "### Discussion\n",
    "The groups are now formed from the unique combinations of values in the two named columns.\n",
    "This probably (remember we don't have the interpretation for the dataset, so we're guessing) represents how many students completed a module in each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Your turn: write a query, using GROUP BY, that shows how many module marks each student has.\n",
    "query = ''' \n",
    "SELECT <your statement here> \n",
    "FROM <and here> \n",
    "GROUP BY <and here?>; \n",
    "'''\n",
    "result = pysqldf(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "code_folding": [
     0
    ],
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Sample solution\n",
    "query = ''' \n",
    "SELECT Student, COUNT(ModuleCode) \n",
    "FROM mixed_module_data \n",
    "GROUP BY Student; \n",
    "'''\n",
    "result = pysqldf(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# We can apply different aggregate functions to the groups.\n",
    "query = ''' \n",
    "SELECT ModuleCode, COUNT(*), AVG(Mark), Max(Mark), Min(Mark) \n",
    "FROM mixed_module_data \n",
    "GROUP BY ModuleCode; '''\n",
    "result = pysqldf(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# We can apply different aggregate functions to the groups, however they are formed.\n",
    "query = ''' \n",
    "SELECT ModuleCode, Year, COUNT(*), AVG(Mark), Max(Mark), Min(Mark) \n",
    "FROM mixed_module_data \n",
    "GROUP BY ModuleCode, Year; \n",
    "'''\n",
    "result = pysqldf(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAVING: conditions on groups\n",
    "SQL also has a HAVING clause which applies the same selection condition to each group formed by a GROUP BY clause.  It's a bit like the WHERE clause, but is applied to groups not rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's pick out only modules with an average mark higher than 50.\n",
    "query = ''' \n",
    "SELECT ModuleCode \n",
    "FROM mixed_module_data \n",
    "GROUP BY ModuleCode\n",
    "HAVING AVG(mark) > 50; \n",
    "'''\n",
    "result = pysqldf(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# The HAVING clause applies to the group, so the condition will \n",
    "# depend on aggregate functions, or is applied to the grouping columns: \n",
    "# it can't refer to individual rows of data, just the groups.\n",
    "query = ''' \n",
    "SELECT ModuleCode \n",
    "FROM mixed_module_data \n",
    "GROUP BY ModuleCode\n",
    "HAVING AVG(mark) > 50 and ModuleCode LIKE ('%4'); \n",
    "'''\n",
    "# recall the LIKE condition is a string pattern matching operator - \n",
    "# this LIKE condition is true for any module code that ends in a 4.\n",
    "result = pysqldf(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUP BY and HAVING in more complex SQL\n",
    "HAVING only makes sense if there is a GROUP BY clause in the SQL statement - you can't apply a condition to a group if there are no groups.\n",
    "But GROUP BY can appear with other SELECT clauses - WHERE and ORDER BY, for example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "### Exercise\n",
    "What do you think the output from the following query represents for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Complex use of grouping.\n",
    "query = ''' \n",
    "SELECT 'above' as 'banding', ModuleCode, count(student), AVG(mark) \n",
    "FROM mixed_module_data \n",
    "WHERE mark >= 40 \n",
    "GROUP BY ModuleCode \n",
    "UNION \n",
    "SELECT 'below', ModuleCode, count(student), AVG(mark) \n",
    "FROM mixed_module_data \n",
    "WHERE mark < 40 \n",
    "GROUP BY ModuleCode \n",
    "ORDER BY ModuleCode; \n",
    "'''\n",
    "result = pysqldf(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "### Discussion\n",
    "The data shows, for each module, the number of students above/below 40 marks and the average mark for those students on that module.\n",
    "\n",
    "The `banding = 'above'` rows are data about those students who achieved at least 40 marks.\n",
    "\n",
    "The `banding = 'below'` rows are data about those students who got less than 40 marks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary: SQL GROUP BY and HAVING\n",
    "SQL tables can be segmented based on the values in one or more named columns using the GROUP BY clause.\n",
    "\n",
    "The HAVING clause applies a condition to each group (segment) which, like the WHERE applied to rows, determines if that group appears in the resulting table.\n",
    "\n",
    "Once grouped, the table cannot be 'ungrouped'; so individual rows are lost and results represent one row per group. This means that the SELECT statement should only contain the grouping column names, aggregate functions, or constant values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split-apply-combine in _pandas_\n",
    "\n",
    "The _pandas_ library (as we've discovered) offers much richer data manipulation than SQL.  \n",
    "\n",
    "In _pandas_ we can do what the SQL GROUP BY and HAVING clauses do.  We can also go further than that allowing groups to be formed from more than just the values in the DataFrame.  We can, for example, apply a function to column values to determine groupings and we can choose to group by rows or by columns.  There is also a wide range of ways to manipulate and control the indexing applied to the data collections that are returned from the grouping method.  Unfortunately, in this module, we can only scratch the surface of what is available in the _pandas_ grouped data structures.\n",
    "\n",
    "In this Notebook we'll look at the grouping using columns from a DataFrame, at the equivalent of the GROUP BY, aggregation and HAVING in SQL, and also look at a feature not easily achieved in SQL, the ability to transform the individual rows of a group based on properties of the group.\n",
    "\n",
    "There are several online tutorials covering the wider application of the _pandas_ grouping, and the usual _pandas_ documention gives several examples. \n",
    "\n",
    "Let's start with looking at what happens when we apply the `groupby()` method to a DataFrame, before looking in more detail at the grouped objects returned.\n",
    "\n",
    "We still have the `mixed_module_data` DataFrame we read in earlier - so that will be our sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The `groupby()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's apply groupby over the ModuleCode column and see what we get.\n",
    "groupeddata = mixed_module_data.groupby(['ModuleCode'])\n",
    "groupeddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Hmmm, the result is too complex just to display the data \n",
    "#            structure - so pandas tells us it is an object.\n",
    "\n",
    "# What is returned is actually a list of two-valued-tuples, where the \n",
    "# first value is the group 'key' value, \n",
    "# and the second value is a DataFrame of the rows in that group.\n",
    "\n",
    "# So we can pull those two values out with a loop construct\n",
    "for groupkey, grouprows in groupeddata:\n",
    "    print(groupkey)\n",
    "    print(grouprows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SQL we had lost the ability to see the rows in the group at this point. The object returned by `groupby()` retains those rows until we decide what we want to do with them.  \n",
    "\n",
    "Often we're going to let the object methods handle the processing of the groups - we can look to see what methods can be applied to the `groupeddata` object that was returned above, using the `<tab>` facility after the `<objectname>`.  In the next cell type `<tab>` after the `.`, then choose the `groups` attribute - this shows us the group keys, and a list of the original index rows that are in that group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "groupeddata.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# And we can pick out a named group, getting a DataFrame as output.\n",
    "groupeddata.get_group('QQ225')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also supply a list of columns to use as the key for the `groupby()` and this forms a composite grouping key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "twolevelgroupeddata = mixed_module_data.groupby(['ModuleCode','Year'])\n",
    "twolevelgroupeddata.groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `groupby()` and aggregations\n",
    "There are two ways to apply aggregate functions to grouped data as the `groupby()` is applied, and,  using the `aggregate()` or `agg()` method on the grouped result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "mixed_module_data.groupby(['ModuleCode']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "groupeddata.aggregate('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is only applied to those rows where it makes sense to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#mixed_module_data.groupby(['ModuleCode']).sum()\n",
    "groupeddata.aggregate('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Or we can pick out the column on which to apply the aggregate \n",
    "#   function in the usual pandas way.\n",
    "groupeddata['Mark'].aggregate('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# The same for multi-column keys too.\n",
    "twolevelgroupeddata['Mark'].aggregate('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `aggregate()` method gives us an ability to fine-tune the application of aggregate functions to columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# More than one function applied to each column.\n",
    "groupeddata.aggregate(['max','min','mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because there are multiple results columns involved, the result has a hierarchical column index.   If only a single non-key column is the target then the column label goes away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "groupeddata['Mark'].agg(['max','min','mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# We can give names to the resulting columns: using a dict where \n",
    "#    the dict key becomes the column name, \n",
    "#    the dict value is the function name.\n",
    "groupeddata['Mark'].agg({'top mark':'max', \n",
    "                         'low mark':'min',\n",
    "                         'average mark':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# And we can apply different functions to different columns.\n",
    "groupeddata.agg({'Mark':{'top mark':'max'}, \n",
    "                 'Year':{'first year':'min'}, \n",
    "                 'Student':'max'})\n",
    "# Each function is applied only to the named column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can use `describe()` on the grouped result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "groupeddata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# And plot() generates a plot for each group.\n",
    "#           BUT you have to be sure to pick out meaningful x and y columns.\n",
    "# CAREFUL; that's one PLOT per GROUP: that's a lot of plots if you have a lot of groups!\n",
    "\n",
    "groupeddata.plot.bar(x='Student', y=['Mark'], ylim=(0,100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `filter()`: _pandas_' version of HAVING,\n",
    "## but not quite\n",
    "In SQL the HAVING clause acted as a filter on the groups: if a group returned True for the condition in the HAVING clause then the group appeared in the result.\n",
    "\n",
    "_pandas_ has the `filter(<a function applied to a group returning True or False\\>)` method which applies the function to the group, **BUT** this returns the original rows from the groups that satisfy the function.  \n",
    "\n",
    "So what you get back from `filter()` is a new DataFrame with only the rows from the original DataFrame that were in the groups that satisfy the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# We need to define the function, over the specific columns of the group, \n",
    "# so it will act as the filter function - i.e. return true or false:\n",
    "filter_average_mark_over_50 = lambda x: (x['Mark'].mean() > 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "groupeddata.filter(filter_average_mark_over_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# It might be easier to see what is happening with a smaller result.\n",
    "# This function is true if a group has fewer than five members \n",
    "#      (x is the group, len(x) is how many elements in the group):\n",
    "filter_small_modules = lambda x: len(x) < 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# So the following reports the rows of any modules that have fewer than five students\n",
    "smallmodulerows = groupeddata.filter(filter_small_modules)\n",
    "smallmodulerows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# If we want the groups that these represent, we need to reapply the groupby after the filter:\n",
    "groupeddata.filter(filter_average_mark_over_50).groupby(['ModuleCode']).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "groupedsmallmodules = smallmodulerows.groupby(['ModuleCode'])\n",
    "groupedsmallmodules.groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `transform()` \n",
    "### Altering the rows of a group based on a group condition or group value\n",
    "\n",
    "This is something SQL can't do without a bit of additional work: affect the rows in a group, based on a property of the group.  \n",
    "\n",
    "`transform()` _does not change_ the original data.  Instead it generates a DataFrame based on the original, without the grouping columns and with updated column values, with one row for each row in the original and the corresponding rows retaining the same index values as the original dataset.  \n",
    "We therefore know that rows in the original and the rows in the transformed columns are aligned by index values.\n",
    "\n",
    "`.transform()` applies the transform function to EVERY column of the DataFrame - so you need to make sure the transform is only applied to meaningful columns of the grouped DataFrame.\n",
    "\n",
    "### Example\n",
    "Suppose we don't like modules where the average mark is below some threshold, and we'd like to adjust student marks if the module average is below that mark.\n",
    "\n",
    "So, how can we ensure the average mark for a group of students on a module is always at least 50?\n",
    "We start by grouping against the `ModuleCode`, then we find each group's mean. \n",
    "\n",
    "If the group mean is below 50 we want to adjust each student mark for students in that group so the group mean is 50.  This gives us the adjusted marks. We then take the adjusted marks result and add that column back to the original data, with a sensible name.   \n",
    "\n",
    "If the group mean is 50 or above then for all students in that group we want to copy across the student's original mark untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# First we need a transformation function that \n",
    "#    returns \n",
    "#         the original individual Mark if the group average is 50 or above, \n",
    "#    and returns  \n",
    "#         (50 - group average) + individual Mark \n",
    "#                                        if the group average is below 50\n",
    "def make_mean_50_if_below(x):\n",
    "    if x.mean() >= 50:  # These groups are OK.\n",
    "        return x\n",
    "    else:               # These groups need to be adjusted.\n",
    "        return (50-(x.mean())) + x\n",
    "\n",
    "# When reading the above you need to remember that x will be a Series of individual student marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# We group the original mixed module data, \n",
    "#    remembering only to include the group key (ModuleCode), \n",
    "#    and the column (Mark) we want to transform.\n",
    "#    Also remember the index column is carried from the original mixed_module_data DataFrame.\n",
    "groupedoriginal = mixed_module_data[['ModuleCode','Mark']].groupby(['ModuleCode'])\n",
    "groupedoriginal.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Now create a new DataFrame with the updated rows based on the transform function's output. \n",
    "rowsupdated = groupedoriginal.transform(make_mean_50_if_below)\n",
    "rowsupdated.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Now we want the above Series to be added to the original data, showing the adjusted values.\n",
    "# Make a copy of the original dataset; we could change the original dataset directly, \n",
    "#                                      but this way keeps the original data clean.\n",
    "updateddata = mixed_module_data.copy()\n",
    "\n",
    "# Add the transformed Mark as a new column,\n",
    "updateddata['UpdatedMark'] = rowsupdated['Mark']\n",
    "\n",
    "# and regroup,\n",
    "groupedupdateddata = updateddata.groupby('ModuleCode')\n",
    "# and a check that the groups have the same rows (we haven't lost or gained anything).\n",
    "groupedupdateddata.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# A quick plot of each group showing the mark and updated mark will show \n",
    "# that some groups' values are transformed, and others aren't.\n",
    "groupedupdateddata.plot.bar(x='Student', y=['Mark','UpdatedMark'])\n",
    "\n",
    "# (hmm, for some reason the first plot is repeated - probably need to report\n",
    "#  this bug to the developers!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# And finally, to confirm the average transformed mark for each group is now 50 or above\n",
    "# and the original Mark is unchanged\n",
    "groupedupdateddata['UpdatedMark','Mark'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The transform function is applied to _every_ column where it is meaningful, according to the datatypes.\n",
    "\n",
    "Here's what happens if we set the group average mark quite high (say 3000!) and apply this function to the whole original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def make_mean_3000_if_below(x):\n",
    "    if x.mean() >= 3000:\n",
    "        return x\n",
    "    else:\n",
    "        return (3000-(x.mean())) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sillygroupedoriginal = mixed_module_data.groupby(['ModuleCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sillyrowsupdated = sillygroupedoriginal.transform(make_mean_3000_if_below)\n",
    "sillyrowsupdated.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you can see, since `Year` is numeric, and the average of the `Year` in each group is about 2015, then the year gets transformed too!  As the `Student` code is non-numeric the `transform()` is not applied to that column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary: _pandas_ split_apply_combine\n",
    "_pandas_ allows DataFrames and Series to be grouped, and the rows within those groups remain accessible.  This allows for a richer range of processing (apply) to build on the result of the split part of split_apply_combine.  \n",
    "\n",
    "As with SQL, aggregate functions are available to summarise groups but unlike SQL the group filtering (`filter()`) returns the rows in the groups that satisfied the condition.  \n",
    "\n",
    "Similarly the ability to transform values based on properties of the group as a whole will return ungrouped data, but the index values ensure transformed data can be aligned to the original datasets.\n",
    "\n",
    "Additional documentation for the _pandas_ `groupby()`, `filter()` and `transform()` can be found at [Group By: split-apply-combine](http://pandas.pydata.org/pandas-docs/stable/groupby.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity",
    "collapsed": true
   },
   "source": [
    "# Exploring some data with grouping, cross tabs and pivot tables\n",
    " \n",
    "You may want to work on one or two of the following to help your understanding of the split_apply_combine processing (and the quick and easy cross tabulation and pivot tables). \n",
    "\n",
    "### You probably don't need to do all of the tasks in the list below.\n",
    "\n",
    "In the `data` folder there is a `salesbook.csv` file.  \n",
    "\n",
    "It's a fairly boring sales ledger of sales records showing for each date (watch the format!) the location of the sales team member, the sales person's name, their sales team, and what was sold (how many and at what unit price).  Note that each salesperson is in only one team, and no team has two or more people with the same name.\n",
    "\n",
    "This would be a good time to explore the OpenStudio facilities to share Notebooks for comment by other students.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "## Your tasks \n",
    "\n",
    "Load the DataFrame from the `CSV` file.\n",
    "\n",
    "You might want to clean up the data a bit. One suggestion is to split the month/day/year, or create functions to do it. (All sales are in 2015 so we could drop that info and we're not interested in the day of the month of the sale, but we are going to be interested in monthly sales - so we want to retain the month values.)\n",
    "\n",
    "You might also want to add a sale amount column (units \\* item cost) to save recalculating each time it's required, as we will be using this value later.\n",
    "\n",
    "Given the above data, and the ability to group datasets, write code to:\n",
    "\n",
    "- a) Show a count of the number of sales records for each District.\n",
    "- b) Show a count of the number of sales records for each Team in each District, including the Team and District margin totals.\n",
    "- c) Show the total sales value for each Team in each District summed over the year.\n",
    "- d) Show the total sales value for each Team Member in each District over the year, showing the District and Team member margin totals. (Remember you need the team name and salesperson name to identify each person uniquely.)\n",
    "- e) Show a bar chart of the number of sales each month. \n",
    "- f) Show a bar chart of the total sales each month.\n",
    "- g) Show a scatter plot showing the Item Cost v. the number of Units in each record.\n",
    "- h) Add a Season column to the DataFrame. For each sale record, the value for Season will be derived from the month: (11,12,1) are Winter, (2,3,4) are Spring, (5,6,7) are Summer, (8,9,10) are Autumn. From the sales in each Season calculate the number, average, maximum, minimum and total sale amount over the season (that is, from all the sales records grouped by season report the number of records, and the average, maximum, minimum and total sales amounts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Start by reading in the CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Now add the SaleAmount column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Now add a Month column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "- a) Show a count of the number of sales records for each District.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "- b) Show a count of the number of sales records for each Team in each District, including the Team and District margin totals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "- c) Show the total sales value for each District summed over the year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "- d) Show the total sales value for each Team in each District over the year, showing the District and Team margin totals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "- e) Show a bar chart of the number of sales each month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "- f) Show a bar chart of the total sales each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "- g) Show a scatter plot showing the Item Cost v. the number of Units in each sales record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "- h) Add a Season column to the DataFrame. For each sale record, the value for Season will be derived from the month: (11,12,1) are Winter, (2,3,4) are Spring, (5,6,7) are Summer, (8,9,10) are Autumn. From the sales in each Season calculate the number, average, maximum, minimum and total sale amount over the season (that is, from all the sales records grouped by season report the number of records, and the average, maximum, minimum and total sales amounts).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "### Discussion\n",
    "Possible solutions for these tasks is given in the Notebook `04.5.soln SalesTeamExploration`; remember there may be more than one way to get the above results but the results should be the same.  \n",
    "\n",
    "When you get a working solution you might want to share it in OpenStudio, and compare your solution with how other students solved the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working through this Notebook as part of an inline exercise, return to the module materials now.\n",
    "\n",
    "If you are working through this set of Notebooks as a whole, move on to `04.6 Introducing regular expressions`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
