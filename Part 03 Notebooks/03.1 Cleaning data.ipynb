{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook you will start to learn about some of the ways in which we can start to clean a dataset programmatically.\n",
    "\n",
    "There is no way we can hope to be exhaustive! Instead, this is intended as a glimpse only, although hopefully enough of a glimpse to enable you to get started on working with your own dirty datasets.  Indeed, some of the examples given in this Notebook are either stubs, or fragmentary notes only - you are encouraged to expand on them as you find you need to use them and learn more about them.\n",
    "\n",
    "This Notebook is intend to be skimmed rather than studied in depth, giving you hints to some common tricks and tips for cleaning data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you start working with real datasets, you are likely to encounter problems with the data that will mean it needs cleaning. This Notebook can act as a first point of reference if you encounter a new problem or issue. As you develop your own data-cleaning habits, it makes sense to keep a note of them somewhere. Feel free to extend this Notebook with your own examples of useful data-cleaning tips and tricks, copy and extend this Notebook on a per-topic basis, or create one or more of your own Notebooks to record your notes. You will gradually produce a Notebook covering a range of approaches and techniques you can apply when working on new projects.\n",
    "\n",
    "For cleaning problems regarding *character and file encodings*, refer to the Notebook `02.2.0 Data file formats - file encodings`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coping with whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will often find that strings start or end with unwanted whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "coursedata_df = pd.DataFrame({ 'coursecode': [' TM351', 'TU100 ', ' M269 '],\n",
    "                               'points': [30, 60, 30],\n",
    "                               'level': ['3', '1', '2']\n",
    "                              })\n",
    "# Pull out the course codes as a list, and then join them with underscores to show the spaces.\n",
    "\"_\".join(coursedata_df.coursecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `strip()` string method to remove whitespace from the start and the end of a string. Alternatively, use `lstrip()` or `rstrip()` to remove whitespace from the left or right-hand end of the string respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "coursedata_df.coursecode = coursedata_df.coursecode.str.strip()\n",
    "\"_\".join(coursedata_df.coursecode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coping with case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the case of elements within a column of a *pandas* DataFrame by applying the `str.upper()` or `str.lower()` method to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "coursedata_df.coursecode.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "coursedata_df.coursecode.str.lower().str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type casting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, we can cast the type of a *pandas* DataFrame column to another type using the `astype()` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Check the datatypes of each column.\n",
    "coursedata_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Here we recast the level and points values to be 64 bit floating point numbers.\n",
    "coursedata_df[ ['level', 'points'] ] = coursedata_df[ ['level', 'points'] ].astype(float)\n",
    "coursedata_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "coursedata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Now we switch the level values to integer type.\n",
    "coursedata_df.level = coursedata_df.level.astype(int)\n",
    "coursedata_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also several *pandas* methods available to handle cases where collections contain mixed types. For example, if you need to cast a Series or DataFrame column to a numeric type, but there are likely to be some elements that aren't castable and need replacing with `NaN` (the not-a-number marker), use `pd.to_numeric()` with the `errors='coerce'` parameter to generate `NaN` for those values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rounding numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you may be presented with floating-point numbers that have lost precision, such as financial amounts that should run to pennies but appear as something like 1.0000001, rather than 1.00.\n",
    "\n",
    "The `round(value, precision)` function will round the value to the nearest value at a specified value of precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Round to 2 decimal places.\n",
    "round(157248.22334673467, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `precision` is not specified, then `round()` rounds to the nearest whole number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Round to an integer value.\n",
    "round(157248.22334673467)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `precision` is a negative value, `round()` interprets the value as a power of ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Round to the nearest thousand (10^3).\n",
    "round(157248.22334673467, -3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data in one column across several columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example is taken from the website 'Stack Overflow' question [Pandas Dataframe: split column into multiple columns, right-align inconsistent cell entries](http://stackoverflow.com/questions/23317342/pandas-dataframe-split-column-into-multiple-columns-right-align-inconsistent-c).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "addresses_df = pd.DataFrame( \n",
    "    {'City, State, Country':['HUN', 'ESP', 'GBR', 'ESP', 'FRA', 'ID, USA', 'GA, USA',\n",
    "                             'Hoboken, NJ, USA', 'NJ, USA', 'AUS'] })\n",
    "addresses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all rows contain a country code, some also have a state with the country, and some have a city, a state and a country. \n",
    "\n",
    "Suppose we want to reshape this as three columns in a DataFrame - one column each for country, state and city.\n",
    "\n",
    "We can split the cell entries on the comma character by applying a string `split()` method to each string in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Split each cell entry on the comma, and assign to a new Series: \n",
    "columnssplitter = lambda x: pd.Series([i for i in (x.split(','))])\n",
    "splitaddresses_df = addresses_df['City, State, Country'].apply(columnssplitter)\n",
    "splitaddresses_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, however, that this is where the original questioner ran into the 'right aligned' problem.  Look at column 0: it's a mix of countries, states and cities.\n",
    "\n",
    "To resolve this, we need to reverse the list of split items, so that countries appear in column 0, states in column 1 and cities in column 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Split each cell entry on the comma, reverse the split list, and assign to new Series columns.\n",
    "splitter = lambda x: pd.Series([i for i in reversed(x.split(','))])\n",
    "splitaddresses_df = addresses_df['City, State, Country'].apply(splitter)\n",
    "splitaddresses_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Now rename the columns.\n",
    "splitaddresses_df.rename(columns = {0:'Country',1:'State',2:'City'}, inplace=True)\n",
    "splitaddresses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques for recognising and parsing time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to work with time-related objects *as time-based data* is a very powerful technique. But first this means we need to be able to recognise strings as representing time, date or datetime objects.\n",
    "\n",
    "Many programming languages offer libraries that support the parsing of time related strings as date, time or datetime objects. Different time elements (for example, day of the week, month of the year, hour of the day in 12 or 24-hour clock format, along with PM or AM modifier) can be parsed using conventional directives.\n",
    "\n",
    "Let's look at one or two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame containing some date and datetime data.\n",
    "timedata_df = pd.DataFrame( \n",
    "            { 'item': ['A','B','C'],\n",
    "              'date':['12-5-12','30-08-11','17-10-10'],\n",
    "              'datetime':['May 7, 2010, 11.14','April 22, 2011, 22.06','October 7, 2013, 00.01']\n",
    "             } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# There's nothing up my sleeves ... as you can see these are simply stored as strings\n",
    "timedata_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# We can cast a column to a datetime object by specifying the way \n",
    "#    the date or datetime string element is formatted.\n",
    "\n",
    "# In this case, we parse a date.\n",
    "pd.to_datetime(timedata_df.date, format='%d-%m-%y')\n",
    "\n",
    "# In this format string we are saying that the date column uses the format day-month-year\n",
    "# The result shows this converted to a datetime datatype, in which datetime elements are\n",
    "# displayed as year-month-day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Here's another example: this time we parse a date and a time.\n",
    "pd.to_datetime(timedata_df.datetime, format='%B %d, %Y, %H.%M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common datetime format elements are:\n",
    "\n",
    "    %a - The abbreviated weekday name (e.g. 'Sun')\n",
    "    %A - The  full  weekday  name (e.g. 'Sunday')\n",
    "    %b - The abbreviated month name (e.g. 'Jan')\n",
    "    %B - The  full  month  name (e.g. `January')\n",
    "    %d - Day of the month (01..31)\n",
    "    %H - Hour of the day, 24-hour clock (00..23)\n",
    "    %I - Hour of the day, 12-hour clock (01..12)\n",
    "    %j - Day of the year (001..366)\n",
    "    %m - Month of the year (01..12)\n",
    "    %M - Minute of the hour (00..59)\n",
    "    %p - Meridian indicator (e.g. 'AM' or 'PM')\n",
    "    %S - Second of the minute (00..60)\n",
    "    %U - Week number of the current year, starting with the first Sunday as the first day of the first week (00..53)\n",
    "    %W - Week number of the current year, starting with the first Monday as the first day of the first week (00..53)\n",
    "    %w - Day of the week (Sunday is 0, 0..6)\n",
    "    %y - Year without a century (00..99)\n",
    "    %Y - Year with century (e.g. 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "For a full list of time-related codes, see the [Python's strftime directives](http://strftime.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a string is not matched by the formatter an error will be thrown. You can force unmatched strings to the `NaT` (*not a time*) value by setting `errors=coerce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame containing something that is not a date.\n",
    "timedata2_df = pd.DataFrame( \n",
    "            { 'item': ['A','B','C'],\n",
    "              'date':['66-65-64','30-08-11','17-10-10'],\n",
    "             } )\n",
    "\n",
    "pd.to_datetime(timedata2_df.date, format='%d-%m-%y', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pandas* can also parse dates and datetimes when reading in dates from CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For general information on handling time in *pandas*, see the *pandas* documentation: [Time Series / Date functionality](http://pandas.pydata.org/pandas-docs/stable/timeseries.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "### Exercise\n",
    "Try making up some of your own date/time strings and see if you can cast them to datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR EXAMPLES HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "What would you do if a date included dates in the form *7th* or *22nd*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A glimpse at regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Regular expressions* will be covered in detail in Part 4 of the module when we consider non-numeric data analysis.\n",
    "\n",
    "Regular expressions are included here as they are a very valuable way of constructing patterns to recognise specific dirty data appearing in strings, and to apply the results of the pattern recognition to rebuild a cleaner string.\n",
    "\n",
    "Regular expressions are another area where we could give a book load of examples, but instead we will show just two:\n",
    "\n",
    "- a method for cleaning alphabetic characters from a numeric value\n",
    "- a method for extracting elements from a string.\n",
    "\n",
    "As you come up with your own useful regular expression cleaning tricks and examples, feel free to add them to this Notebook, and you can share them on the module forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning alphabetic characters from a numeric value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's tidy up the following number representations.\n",
    "messynumbers_df = pd.DataFrame({'messyvals': ['£40000', \"UKP 25,000\", '25000 pounds Sterling'] })\n",
    "messynumbers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# First remove any commas:\n",
    "messynumbers_df['cleanvals'] = messynumbers_df.messyvals.str.replace(',', '')\n",
    "messynumbers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Now apply a regular expression to get rid of the non-numeric characters \n",
    "#   left and right of the digits we want to keep.\n",
    "# The bracketed term '([\\d]*)' in the middle of the complex pattern string \n",
    "#   is the one we are extracting as '\\1'.\n",
    "messynumbers_df.replace({'cleanvals' : \"^[^\\d]*([\\d]*)[^\\d]*$\"}, {'cleanvals' : r'\\1'}, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Notebook on regular expressions in Part 4 you will be shown how this regular expression is shaped.\n",
    "\n",
    "This is the regular expression part:   `\"^[^\\d]*([\\d]*)[^\\d]*$\"` \n",
    "\n",
    "It reads: match the start of the string (`^`) followed by zero or more (`*`) occurrences of any non-digit characters `[^\\d]`, then match as a usable pattern `()` any string of zero or more (`*`) digits `[\\d]`, followed by zero or more (`*`) non-digit characters `[^\\d]` and the end of the string (`$`)\n",
    "\n",
    "The 'usuable' pattern is then referenced as `\\1` in the replacement string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting elements from a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example we are given a list of web pages; the plan is to extract the domain into a new column and the filetype into a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "urls_df = pd.DataFrame({'url':['http://this.example.com/path/file.html',\n",
    "                               'http://another.example.com/longer/path/file.json']})\n",
    "urls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's create new columns for each of the extracts, based on the original:\n",
    "urls_df['domain'] = urls_df['url']\n",
    "urls_df['filetype'] = urls_df['url']\n",
    "\n",
    "# We can pull out the first item - the domain - in this simple example easily enough:\n",
    "# Simply find everything between the http:// and the next /\n",
    "urls_df.replace({'domain' : \"^http://([^/]*).*$\"}, {'domain' : r'\\1'} , regex=True, inplace=True)\n",
    "\n",
    "# We could extend the same regular expression, with a match term for everything after the last '.'\n",
    "# This second match term is the one we extract to give the filetype:\n",
    "urls_df.replace({'filetype' : \"^http://([^/]*).*\\.([^\\.]*)$\"}, {'filetype' : r'\\2'},\n",
    "                regex=True, inplace=True)\n",
    "urls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet shows how we can call on both those extracted values and reorder them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "urls_df.url.replace(\"^http://([^/]*).*\\.([^\\.]*)$\", r'We got a(n) \\2 file from \\1.', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenRefine application provides a set of clustering tools that attempt to group together partially matching strings in order to support data normalisation as part of a data-cleaning process. \n",
    "\n",
    "Note that the OpenRefine service publishes an API that can be accessed from a Notebook, although we will not explore it in this module. For further information, see https://github.com/PaulMakepeace/refine-client-py/ and this example Notebook: http://nbviewer.ipython.org/gist/trevormunoz/6265360\n",
    "\n",
    "There are several fuzzy matching packages available for Python, such as https://github.com/seatgeek/fuzzywuzzy  or the https://pypi.python.org/pypi/Fuzzy (which includes phonetic matching), as described in http://www.informit.com/articles/article.aspx?p=1848528.\n",
    "\n",
    "Other than noting the existence of these libraries, we will not explicilty explore them further in this module, although you are welcome to use them in your own data explorations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, you have seen examples of several data-cleaning techniques, albeit in quite a cursory form.\n",
    "\n",
    "Data cleaning is something that benefits from a build up of case knowledge and experience. Feel free to add to this Notebook as you come up with your own data-cleaning recipes.\n",
    "\n",
    "If you are working through this Notebook as part of an inline exercise, return to the module materials now.\n",
    "\n",
    "If you are working through this set of Notebooks as a whole, move on to `03.2 Selecting and projecting, sorting and limiting`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
