{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining data from multiple datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we'll work through a number of different ways in which two (or more) tabular datasets can be combined into a single table.  \n",
    "\n",
    "In the first instance we will look at forming the 'union' of two tables that have the same structure and common datatypes for the columns. The union of two tables usually represents having the same type of data in two tables - say the attendance register in two school classes, or the crime rates in two police districts - that we want to combine into a single dataset.  This is about adding more rows of the same type of data to the base dataset.\n",
    "\n",
    "The second type of combination of data will be where we have more data about the same things - where we want to add to the data in a row, not simply add more rows.   Two tables of this form usually have some common contents represented in a row, so this is about adding more columns and column-row values to the dataset.  This is known as the 'join' of two tables.  And there are a lot of issues to consider when joining tables (so, as this is quite a long Notebook, you may want to take a break while working through it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The examples used in this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous Notebook on selecting and projecting data from tables, we created and used DataFrames in _pandas_ and used the _pandasql_ library to show the basic operations.  \n",
    "\n",
    "In this Notebook we'll use _pandas_ DataFrames formed from datasets from external sources; we'll explain the origins and form of this data as we go.\n",
    "\n",
    "We'll also take a different approach to the SQL evaluation in this Notebook. We'll use the external Postgres database management system (DBMS) which is an application running outside the Python Notebook.  \n",
    "\n",
    "There are two reasons for this approach:\n",
    "- so that you are familiar with accessing data held and processed in external systems (the _it's good for you_ reason!)\n",
    "- the pandasql library makes use of an sqlite database engine which doesn't have all the join types we will consider (the _pragmatic_ reason!).\n",
    "\n",
    "The Notebook takes you through the SQL examples first, then returns to them to look at how _pandas_ achieves the same (or similar) results.  But this shouldn't stop you starting with the _pandas_ if you want, as long as you then come back to the SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the Postgres database engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The virtual machine you're using on this module has a PostgresSQL database management system installed.\n",
    "\n",
    "We'll be using this to run the SQL code in this Notebook.   To do this we need first to connect to the Postgres system, and then have  a way to tell Python to pass the SQL code to the Postgres system for evaluation and to copy back into the Notebook any results tables we wish to capture.\n",
    "\n",
    "This is most easily done using SQL cell magic - it's a way of marking a cell as containing SQL code and the Notebook will route the SQL to the connected DBMS. SQL cell magic cells start with `%sql`.\n",
    "\n",
    "The exact details of the connection to the DBMS depend on the DBMS in use.   However, the following works for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Load in the sql extensions:\n",
    "%load_ext sql\n",
    "\n",
    "\n",
    "# Then connect to a Postgres SQL database.\n",
    "%sql postgresql://test:test@localhost:5432/tm351test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the *sql magic* extension loaded, we start a cell with `%%sql` and then write SQL commands. \n",
    "\n",
    "The following cell begins with the sql magic marker `%%sql`. It then checks to see if the `quickdemo` table already exists: if it does, it removes it (in SQL terms it _drops_ it).  Then it _creates_ the `quickdemo` table with three columns, called `id`, `name` and `value`, before _inserting_ two rows of data into the table.\n",
    "\n",
    "Notice that this is  a cell with five SQL commands in it, each ending in ';'. The `%%sql` magic marker indicates the **whole** cell is to be processed by the connected DBMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS quickdemo;\n",
    "CREATE TABLE quickdemo(id INT PRIMARY KEY, name VARCHAR(20), value INT);\n",
    "INSERT INTO quickdemo VALUES(1,'This',12);\n",
    "INSERT INTO quickdemo VALUES(2,'That',345);\n",
    "\n",
    "SELECT * FROM quickdemo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not possible to put a Python assignment statement at the end of a `%%sql` cell, so the table returned at the end of the cell (the `SELECT * FROM quickdemo;`) is picked up in the following cell using the `_` variable.  \n",
    "\n",
    "The tables returned are lists - but we can use the result's `DataFrame()` method to convert this to a DataFrame, complete with index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "result_from_sql = _\n",
    "result_from_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "result_df = result_from_sql.DataFrame()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a code cell it is also possible to use `%sql` ahead of a _single_-line SQL command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "result = %sql SELECT * FROM quickdemo WHERE value > 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "dataframe_df = result.DataFrame()\n",
    "dataframe_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SQL: more of the same, the UNION of multiple datasets\n",
    "## Vertical joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here are two tables ABCD1 and ABCD2 with a few rows of data in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS ABCD1;  -- This just allows us to run this cell repeatedly, \n",
    "                             -- it destroys the table before we recreate it;\n",
    "                             -- this is not the normal way to use SQL where the \n",
    "                             -- persistence of data is important.\n",
    "CREATE TABLE ABCD1(a CHAR(2), b CHAR(2), c CHAR(2), d CHAR(2) );\n",
    "INSERT INTO ABCD1 VALUES('A1','b1','c1','d3');\n",
    "INSERT INTO ABCD1 VALUES('A1','b1','c1','d4');\n",
    "\n",
    "\n",
    "SELECT * FROM ABCD1; -- The '--' represents an in-line comment to SQL: \n",
    "                     -- anything after the '--' on the line is ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS ABCD2;  \n",
    "\n",
    "CREATE TABLE ABCD2(a CHAR(2), b CHAR(2), c CHAR(2), d CHAR(2) );\n",
    "INSERT INTO ABCD2 VALUES('A2','b2','c2','d1');\n",
    "INSERT INTO ABCD2 VALUES('A1','b1','c2','d7');\n",
    "INSERT INTO ABCD2 VALUES('A6','b1','c8','d6');\n",
    "\n",
    "SELECT * FROM ABCD2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the two tables have the same structure, the same number of columns and same datatypes in each column. They also have the same column names which is handy as we're trying to give the impression that the two tables are of data representing the same types of things.   \n",
    "\n",
    "The vertical join of these two tables is achieved with the SQL UNION clause between two SELECT statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT a,b,c,d\n",
    "FROM ABCD1\n",
    "UNION\n",
    "SELECT a,b,c,d\n",
    "FROM ABCD2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "Notice that the table headers are metadata, so are not repeated, but are inherited by default from the first SQL statement.\n",
    "(Also SQL doesn't maintain the order of the original rows.)  It also doesn't really care about the column names - just that the datatypes are compatible and the number of columns in each SELECT is the same. This is known as having **union-compatible tables**: the same number of columns and compatible datatypes for the columns.\n",
    "\n",
    "We can reshape the original tables, using projection and selection, if we need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT a,b,c\n",
    "FROM ABCD1\n",
    "WHERE d='d4'\n",
    "UNION\n",
    "SELECT a,b,c\n",
    "FROM ABCD2\n",
    "WHERE c='c2';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the tables to force compatability\n",
    "The above abstract example, with all the columns having the same datatype and no real semantics behind the values shown, might give the impression that not much work needs to be done to ensure the two tables are compatible - but union-compatible doesn't necessarily mean semantically compatible.\n",
    "\n",
    "Consider the following two tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS Parts1;  \n",
    "\n",
    "CREATE TABLE Parts1(description VARCHAR(20), \n",
    "                    length_in_cm REAL, \n",
    "                    colour VARCHAR(20) );\n",
    "\n",
    "INSERT INTO Parts1 VALUES('Plank',160.0,'Oak');\n",
    "INSERT INTO Parts1 VALUES('Brace',20.2,'Green');\n",
    "\n",
    "SELECT * FROM Parts1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS Parts2;  \n",
    "\n",
    "CREATE TABLE Parts2(description VARCHAR(20), \n",
    "                    length_in_metres REAL );\n",
    "\n",
    "INSERT INTO Parts2 VALUES('Flange',0.5);\n",
    "INSERT INTO Parts2 VALUES('Sprocket',2.4);\n",
    "\n",
    "SELECT * FROM Parts2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only do they have different numbers of columns, but the interpretation of the length columns would suggest they're not actually compatible, even though the underlying type is REAL in both cases.  So here, if we want to union the two tables we've got some **harmonisation** to do first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT description, length_in_cm, colour\n",
    "FROM Parts1\n",
    "UNION\n",
    "SELECT description, (length_in_metres*100.0), NULL\n",
    "FROM Parts2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear, something odd has happened here.  \n",
    "\n",
    "The table looks to be the correct shape - it has the three columns we expect, and all the rows we want to see. But some of the values are not quite what we would expect. \n",
    "\n",
    "(1) The `NULL` marker in SQL is used to show where a value doesn't apply to a row; so we used it in the SQL code for the Parts 2 colour column values in the unioned table.  However, this has been replaced by `None` in the displayed result (that looks to be a Python side-effect).\n",
    "\n",
    "(2) The Postgres SQL is showing that floating-point arithmetic can sometimes become inaccurate due to the precision of the stored values. If accuracy matters, we could round the resulting values as part of our harmonisation.  Here it's just an annoying detail to note - but in some calculations you really would want to know such things were occuring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking care with vertical joins\n",
    "SQL is usually quite good at enforcing datatypes and ensuring consistency when it can; but other tabular data tools - spreadsheets and text editors - may take a more cavalier approach, allowing confusing hybrid columns to result.\n",
    "\n",
    "We can demonstrate this with the two tables above: SQL sees that the description and colour columns have the same base type and, of course, we could simply neglect to convert the metres to centimetres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT colour, length_in_cm, description\n",
    "FROM Parts1\n",
    "UNION\n",
    "SELECT description, length_in_metres, NULL\n",
    "FROM Parts2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "### Exercise\n",
    "Here is data from two expenses claim sheets from an employee who drives her own car on UK roads, but a hire car when in Europe.\n",
    "\n",
    "            UKMileage\n",
    "\n",
    "EmployeeName | Date | Start Location|Destination|Distance\n",
    "--------------|------|---------------|-----------|-------\n",
    "Smith|10-10-2010|Newcastle|Sunderland|13.1\n",
    "Smith|11-10-2010|Sunderland|Newcastle|14.2\n",
    "\n",
    "\n",
    "            EuropeanMileage\n",
    "\n",
    "EmployeeName | Date | Start Location|Destination|Distance\n",
    "--------------|------|---------------|-----------|-------\n",
    "Smith|12-12-2010|Rouen|Paris|123.6\n",
    "Smith|13-12-2010|Amiens|Calais|179.3\n",
    "\n",
    "Is it safe to simply UNION these two datasets?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "### Discussion\n",
    "We haven't got sufficient information to tell us. The values in the columns all look reasonably compatible, but we need to understand the semantics behind the values and their origin.   Did the `Distance` values come directly from the cars' odometers?  In this case the UK figures are probably in miles, while the Europe figures are in kilometres.  Or has Smith already applied a conversion?  Without a description of the units, or a company policy document, or by consulting an external data source, such as a route planner, it would be unsafe to UNION the tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SQL: more about the same, joining multiple datasets\n",
    "## Horizontal joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're combining tables in ways that put the columns from the original datasets side by side. In most cases we do this because the two tables contain different information about the same things (represented in each row) and we want to create a single table with the combination of that data appearing in a single row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example,  each year the Higher Education Statistics Authority (HESA) publishes a wide variety of data about the performance of every UK university.  Each year, the UK Higher Education Funding Council (HEFCE) also publishes the results of a National Student Survey, again broken down by university. Data about research grants is published via the Gateway to Research site.  Results of the Research Excellence Framework are published by another organisation, and so on. Each organisation publishs data about the same set of things - UK universities.  So, how would we go about creating one table in which each university had all its data on a single row?\n",
    "\n",
    "This process of combining rows from multiple tables that have the same values in some columns (i.e. University Name) is called the **horizontal join** (usually just **join**). In the following we'll explore several variations on the join, as there are quite a few variations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cartesian product\n",
    "The simplest case of the horizontal join doesn't try to do anything to match values between the source tables. It simply puts every row from one table alongside every row in a second table; the table that results is known by mathematicians as the Cartesian product.  \n",
    "\n",
    "The Cartesian product is of interest because it's the basic logical building block for all the other horizontal joins.  \n",
    "\n",
    "Let's look at an example, using our ABCD1 table from earlier (we'll remind ourselves what it looks like first) and a second similarly arbitrary table XYZ1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM ABCD1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS XYZ1;\n",
    "\n",
    "CREATE TABLE XYZ1(x CHAR(2), y CHAR(2), z CHAR(2) );\n",
    "INSERT INTO XYZ1 VALUES('X1','d3','z1');\n",
    "INSERT INTO XYZ1 VALUES('X2','d3','z2');\n",
    "INSERT INTO XYZ1 VALUES('X3','d4','z3');\n",
    "\n",
    "SELECT * FROM XYZ1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SQL to produce the Cartesian product is simply to list the two (or more) table names in the FROM statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM ABCD1, XYZ1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the resulting table you will see that each row in the original `ABCD1` table is repeated alongside every row in the original `XYZ1` table.  The source tables had 2 and 3 rows, the result has 2 * 3 = 6 rows.\n",
    "\n",
    "In fact the Cartesian product result gets big quite fast, so we rarely use it - but we did say it was the basis of the rest of the horizontal join types so let's look at some more interesting joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The equality and theta joins\n",
    "In these joins a condition is applied between values in the columns of the two tables being joined; only those rows in the Cartesian product that satisfy the condition appear in the result.\n",
    "\n",
    "Usually that's an equality condition, giving us the equality join, but it could be an arbitrary condition between the values. \n",
    "\n",
    "Here's an example of an equality join between the values in the `d` and `y` columns of our two tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM ABCD1 JOIN XYZ1 ON d=y;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this only has rows in which the value of `d` and the value of `y` in the row are the same; it's the equality join expressed using the `ON d=y` part of the statement.\n",
    "\n",
    "A way to remember the behaviour of the equality and theta joins is to think of them as filtering the Cartesian product:\n",
    "\n",
    "    In the FROM statement, form the Cartesian product of ABCD1 and XYZ1,\n",
    "    For each row in the Cartsian product, if the ON condition is TRUE put that row in the result otherwise discard that row,\n",
    "    now SELECT the columns to project into the result from the remaining rows.\n",
    "    \n",
    "Let's look at a slightly more meaningful example - two sets of students and the marks they got on the module they took this year and the module they took last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS this_year;\n",
    "\n",
    "CREATE TABLE this_year(student VARCHAR(20), \n",
    "                       course VARCHAR(20), \n",
    "                       mark INT);\n",
    "\n",
    "INSERT INTO this_year VALUES('Ann','TM351',55);\n",
    "INSERT INTO this_year VALUES('Alison','TM351', 90);\n",
    "INSERT INTO this_year VALUES('Andy','TM355',5);\n",
    "INSERT INTO this_year VALUES('Arthur','TM356',5);\n",
    "\n",
    "SELECT * FROM this_year;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS last_year;\n",
    "\n",
    "CREATE TABLE last_year (name VARCHAR(20), \n",
    "                        module VARCHAR(20), \n",
    "                        score Int);\n",
    "\n",
    "INSERT INTO last_year VALUES('Ann','TM352',40);\n",
    "INSERT INTO last_year VALUES('Alison','TM352', 70);\n",
    "INSERT INTO last_year VALUES('Andy','TM356',90);\n",
    "INSERT INTO last_year VALUES('Arthur','TM356',55);\n",
    "SELECT * FROM last_year;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "### Exercise \n",
    "Write a join that will show you which students sat the same module this year and last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "<put your SQL here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "### Solution\n",
    "In this query you need two equality conditions to be true in the same row - one for the student's name, the other for the module=course values.   This is still *technically* an equality join, but you can see we can put any condition applied to the Cartesian product row into the ON clause - this is where you get the *theta* join - if the condition in the ON is not an equality condition then it's described as a *theta* join.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM  this_year JOIN last_year ON (student = name) AND (course = module);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity"
   },
   "source": [
    "### Exercise\n",
    "Write an SQL statement using a join that will show you which students did better on this year's module than on last year's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "<put your SQL here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "### Solution\n",
    "In this query you're only interested in rows in the Cartesian product which have the same student *and* where the mark they got this year is higher than the score they got last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM  this_year JOIN last_year ON (student = name) AND (mark > score);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencing  the same column name in more than one table: table.column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the above example we deliberately chose column names for the two tables where there was no ambiguity.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS another_last_year;\n",
    "\n",
    "CREATE TABLE another_last_year (student VARCHAR(20), course VARCHAR(20), mark Int);\n",
    "INSERT INTO another_last_year VALUES('Ann','TM352',40);\n",
    "INSERT INTO another_last_year VALUES('Alison','TM352', 70);\n",
    "INSERT INTO another_last_year VALUES('Andy','TM356',90);\n",
    "INSERT INTO another_last_year VALUES('Arthur','TM356',55);\n",
    "SELECT * FROM another_last_year;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we resolve this ambiguity. When two tables have columns with the same name we use the full table and column name combination: `<tablename>.<columnname>` shown below. However, notice what the column names of the resulting table have been changed to in order to avoid confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM  this_year JOIN another_last_year \n",
    "      ON (this_year.student = another_last_year.student) \n",
    "        AND (this_year.mark > another_last_year.mark);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## Natural join\n",
    "It's quite common for two tables to have columns in different tables with the same name - this often indicates that there is a relationship between the rows in the two tables (in relational database terms the values usually represent primary and foreign keys).\n",
    "\n",
    "Here's an example, using data taken from UK Government data tables on car fuel consumption figures - the original data can be found at hhtp://carfueldata.direct.gov.uk/search-by-fuel-economy.aspx (accessed 21-May-2015).  (note the data needed some cleaning and reshaping to get it into the form shown below).\n",
    "\n",
    "The first table shows the combined fuel consumption figures, the second shows the base consumption figures (the Urban and Extra Urban figures).   In both tables the values of the manufacturer, model and fuel-type columns act as a unique key for the rows in the table, and those rows with the same key values in each table have information about the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS car_type_combined_consumption;\n",
    "\n",
    "CREATE TABLE car_type_combined_consumption (manufacturer VARCHAR(30), model VARCHAR(50), imperial_combined REAL, fuel_type VARCHAR(10));\n",
    "INSERT INTO car_type_combined_consumption VALUES('MORGAN MOTOR COMPANY','2000, From January 2011 onwards',40.3,'Petrol');\n",
    "INSERT INTO car_type_combined_consumption VALUES('CHEVROLET','Orlando, MY2013', 40.3, 'Petrol');\n",
    "INSERT INTO car_type_combined_consumption VALUES('VOLKSWAGEN C.V.','California Motor Home',40.4, 'Diesel');\n",
    "INSERT INTO car_type_combined_consumption VALUES('VOLKSWAGEN','Touareg', 40.4, 'Diesel');\n",
    "INSERT INTO car_type_combined_consumption VALUES('VOLKSWAGEN','Passat Saloon',40.4,'Petrol');\n",
    "SELECT * FROM car_type_combined_consumption;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS car_type_base_consumption;\n",
    "\n",
    "CREATE TABLE car_type_base_consumption (manufacturer VARCHAR(30), model VARCHAR(50), fuel_type VARCHAR(10), imperial_urban_cold REAL, imperial_extra_urban REAL);\n",
    "INSERT INTO car_type_base_consumption VALUES('MORGAN MOTOR COMPANY','2000, From January 2011 onwards','Petrol',32.8, 46.3);\n",
    "INSERT INTO car_type_base_consumption VALUES('CHEVROLET','Orlando, MY2013', 'Petrol', 34.4, 44.8);\n",
    "INSERT INTO car_type_base_consumption VALUES('VOLKSWAGEN C.V.','California Motor Home', 'Diesel',29.7, 51.4);\n",
    "INSERT INTO car_type_base_consumption VALUES('VOLKSWAGEN','Touareg', 'Diesel',28.5, 55.3);\n",
    "INSERT INTO car_type_base_consumption VALUES('VOLKSWAGEN','Passat Saloon','Petrol',30.7, 49.5);\n",
    "SELECT * FROM car_type_base_consumption;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a **natural join**, two tables must have at least one column in each with the same name and the same datatypes; the result will be an equality join applied to all of those matching columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM car_type_combined_consumption NATURAL JOIN car_type_base_consumption;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to be careful of natural joins: firstly the equality applies to *all* the columns that have the same name and datatype, and secondly if someone adds or removes columns from one of the tables then the behaviour of the query with the natural join may change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer joins\n",
    "The final major form of the join is the outer join - to handle cases when we would lose data if we simply accepted only the joined rows based on their filtering condition.\n",
    "\n",
    "Look back at the join examples we've seen so far:\n",
    "\n",
    "* the Cartesian product joined every row in one table with every row in the other \n",
    "* the other joins have generally been based around a condition (most often equality) between two tables in which values to be matched appeared in both tables. \n",
    "\n",
    "So, in the examples we've seen so far the result rows have always had values from rows taken from both tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider this example:\n",
    "\n",
    "Each year a small sports club registers members' names and addresses with the sports association, which gives them a registration number that the club notes.  The club also collects membership fees, which may be paid in installments, and they keep a running total of the amount paid by each member. At the start of the year this table is empty.\n",
    "\n",
    "At some point the club has two datasets:\n",
    "\n",
    "            member\n",
    "\n",
    "name|address|registration_no\n",
    "----|-------|---------------\n",
    "Kevin | Milton Keynes | R345\n",
    "Katy | Bedford | R34\n",
    "Kirrin | Luton | R45\n",
    "\n",
    "and \n",
    "\n",
    "            payment_made\n",
    "\n",
    "name|total_amount\n",
    "----|------------\n",
    "Katy|54\n",
    "Kevin|33\n",
    "\n",
    "Note that Kirrin is missing from the payment table, as she has just joined and has not paid anything yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS member;\n",
    "\n",
    "CREATE TABLE member (name VARCHAR(30), address VARCHAR(50), registration_no VARCHAR(5));\n",
    "INSERT INTO member VALUES('Kevin','Milton Keynes','R345');\n",
    "INSERT INTO member VALUES('Katy','Bedford', 'R34');\n",
    "INSERT INTO member VALUES('Kirrin','Luton', 'R45');\n",
    "\n",
    "DROP TABLE IF EXISTS payment_made;\n",
    "\n",
    "CREATE TABLE payment_made (name VARCHAR(30), total_amount INT);\n",
    "INSERT INTO payment_made VALUES('Katy', 54);\n",
    "INSERT INTO payment_made VALUES('Kevin', 33);\n",
    "\n",
    "SELECT * FROM member;\n",
    "SELECT * FROM payment_made;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose the manager wants a query to show both registrations and payments in a single table.\n",
    "In the cell below try writing a JOIN using the examples above that will create the table.\n",
    "\n",
    "**Result**\n",
    "\n",
    "name|address|registration_no|total_amount\n",
    "----|-------|---------------|------------\n",
    "...|...|...|...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM member, payment_made;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple Cartesian product makes a real mess, simply putting everyone against everyone else.\n",
    "If you try to use any of the conditional joins you'll notice that we keep losing Kirrin in the result table.   She has a registration number, but has no row in the payment table, so the join conditions never succeed.\n",
    "\n",
    "To include those rows that appear in one table, but with no corresponding row in the other table we use an OUTER join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM member LEFT OUTER JOIN payment_made ON member.name = payment_made.name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LEFT OUTER JOIN`  says that if there is an unmatched row in the left table (the one named before the `LEFT OUTER JOIN` text) then it is kept in the result with NULLs in the columns from the right table.\n",
    "The `None` (which substitutes for the SQL `NULL` marker) shows that there is no value in these row-column intersections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose Kirrin pays £10, but brings along her friend Lucy who immediately pays the club £20 but does not fill in her registration form.  \n",
    "We record these two events in the data tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "INSERT INTO payment_made VALUES('Kirrin',10);\n",
    "INSERT INTO payment_made VALUES('Lucy',30);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kirren will now appear in the equality or natural joins because there is data in both tables against her name, and of course we've seen she appears in the LEFT OUTER JOIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM member NATURAL JOIN payment_made;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM member LEFT OUTER JOIN payment_made ON member.name = payment_made.name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we've lost Lucy's payment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "### Exercise\n",
    "How do you think we can change the query to generate the required result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM member LEFT OUTER JOIN payment_made ON member.name = payment_made.name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "We have two options: \n",
    "\n",
    "(1) reorder the two references to the tables in the `JOIN` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql -- solution\n",
    "SELECT * \n",
    "FROM payment_made LEFT OUTER JOIN member ON member.name = payment_made.name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "(2) or use the `RIGHT OUTER JOIN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql -- solution \n",
    "SELECT * \n",
    "FROM member RIGHT OUTER JOIN payment_made ON member.name = payment_made.name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, by using LEFT or RIGHT OUTER JOINs we can allow 'unmatched' rows from the first or second table in the JOIN statement - but what about unmatched rows from _both_ tables?\n",
    "\n",
    "Suppose Kirrin hadn't paid the £10, but Lucy had paid her £20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DELETE FROM payment_made WHERE name ='Kirrin';\n",
    "SELECT * FROM payment_made;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see both Kirrin, who has no payment noted, and Lucy, who has no registration number, we need a `FULL OUTER JOIN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM member FULL OUTER JOIN payment_made ON member.name = payment_made.name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that there would still be some work to do to make the output of the FULL OUTER JOIN useful.  The `member.name` column has a `NULL` (`None`) in it for Lucy. We would need to copy across the `payment_made.name`, otherwise anyone looking for people by name would need to search two columns of the table.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SQL summary: where we've reached\n",
    "\n",
    "We've been looking at combining into a single table, data from more than one table; so far, we've seen this in SQL.\n",
    "\n",
    "Let's recap what has been covered, before we move on to seeing the same types of table combinations in *pandas*.\n",
    "\n",
    "So far we've seen:\n",
    "1. how to access PostgreSQL from the notebook.\n",
    "2. vertical joins: the union of union-compatible tables\n",
    "3. horizontal joins: cartesian product, equality and theta joins, natural join, and left, right and full outer joins.\n",
    "\n",
    "Let's now move on to looking at how *pandas* handles combining data from multiple DataFrames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *pandas*: combining data from multiple tables\n",
    "To work through the remainder of this Notebook we will use a single example, using data from the Open Data Communities website (details are given below). The datasets are held in the `data` folder for this part of the course.\n",
    "\n",
    "The main sections in the rest of the Notebook correspond to the vertical and horizontal joins.\n",
    "\n",
    "Let's start by describing, then loading, our example datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the datasets\n",
    "The data we will use for this activity comes from the Department for Communities and Local Government Open Data Communities (DCLG) website (http://opendatacommunities.org/). \n",
    "\n",
    "Two sorts of data have been downloaded from this site \n",
    "- information about the average weekly social rent of new PRP (Private Registered Providers) general needs lettings for 2012/13  \n",
    "- data relating to house building, in particular the permanent dwellings started from 2009/10 to 2012/13.\n",
    "\n",
    "We've also copied an Ordnance Survey file giving names for geographical areas and reference codes for them.\n",
    "\n",
    "We've copied the data into files in the `housingdata` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!ls data/housingdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The house building data files all have a similar form (which looks messy due to the wide rows wrapping):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!head data/housingdata/house-building-starts-tenure-2009-2010.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Looking at this data, we can see most of the metadata gives URLs related to the definitions of the terms and concepts relevant to the file content.  This ensures that we can check our interpretation and understanding of the data elements and their context, by reference to the relevant definitions.\n",
    "\n",
    "When we get down to the rows of data, each row has URL link to a reference element for the statistical geograph area, and the Reference code is repeated along with the textual name of the reference area. \n",
    "\n",
    "The first column value gives you the link to Sparql data for the local authority area (you'll cover Sparql in Part 23 of the module). The second column gives you two types of label for the reference area.  The data we're interested in manipulating is in the second and subsequent columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!head data/housingdata/house-building-starts-tenure-2010-2011.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!head data/housingdata/households-social-lettings-general-needs-rents-prp-number-bedrooms-2012-2013.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have also pulled down a file from the Ordnance Survey that contains a list of geographical areas within the Yorkshire and the Humber region, some of which are local councils and some of which aren't. Note that the data that identifies each authority appears to resemble that used in the DCLG data files but does not match exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!head -n 5 data/housingdata/yorksAndHumberside.csv\n",
    "# In the cell output,  the first column (up to the first comma) is a \n",
    "# URL giving access to an Ordnance Survey page for each district."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading the house building data \n",
    "We can load the data from the CSV files using the *pandas* `read_csv()` function. For the housing data, we need to skip the first five lines (I counted!) of the file before accepting the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Read in some of the data.\n",
    "bldg_2009_10_df = pd.read_csv('data/housingdata/house-building-starts-tenure-2009-2010.csv', skiprows=5)\n",
    "bldg_2010_11_df = pd.read_csv('data/housingdata/house-building-starts-tenure-2010-2011.csv', skiprows=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Preview the data we have loaded, to make sure it looks sensible.\n",
    "bldg_2009_10_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR TURN\n",
    "# Import the remaining house building files into separate DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Vertical joins: \n",
    "## concatenating house building data from several datasets\n",
    "\n",
    "Suppose we want to work with a single DataFrame that contains all the annual house building starts data over the period 2009-2013. \n",
    "\n",
    "The pandas `concat()` function will concatenate rows from a list of DataFrames where each DataFrame shares the same column headings.\n",
    "\n",
    "Let's create a couple of samples from the tables just to try this function out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Just use a sample of the data rows for now as we develop the code\n",
    "sample1_df = bldg_2009_10_df[:3]\n",
    "sample2_df = bldg_2010_11_df[:3]\n",
    "sample2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so let's test the `concact()` function on our `sample1_df` and `sample2_df` datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Try out the concat() function - pass in a list of DataFrames to be concatenated.\n",
    "pd.concat([sample1_df, sample2_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "That should have worked OK... We've got the rows from two DataFrames combined into a single DataFrame - the row indexes have been repeated, but the table structure looks OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What happens when we try to merge two complete DataFrames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bldg_2009_11_df = pd.concat([bldg_2009_10_df, bldg_2010_11_df])\n",
    "# Check to see if the dataframes appear to have been concatenated \n",
    "# together by inspecting row counts.\n",
    "print(len(bldg_2009_10_df), len(bldg_2010_11_df), len(bldg_2009_11_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks to have worked, or it did when I tried it! The original DataFrames have 300 and 309 rows each, the merged DataFrame has 609 rows; so no rows appear to have been lost or added. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What happens if the DataFrames have the same column names, but they appear in a different order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a sample DataFrame containing the *same* columns as the original but \n",
    "# in a *different* order.\n",
    "sample3_df = bldg_2009_10_df[['Reference area',\n",
    "                              'All',\n",
    "                              'Housing-Associations','http://opendatacommunities.org/def/ontology/geography/refArea',\n",
    "                              'Local-Authority',\n",
    "                              'Private-Enterprise']][:3]\n",
    "sample3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate DataFrames with the same columns, but differently ordered.\n",
    "concat_difforder_df = pd.concat([sample1_df, sample3_df])\n",
    "concat_difforder_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_pandas_ is capable of automatically aligning the named columns from such DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns",
    "collapsed": true
   },
   "source": [
    "### Exercise\n",
    "How does _pandas_ behaviour compare with SQL when the columns are in different orders?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity",
    "collapsed": true
   },
   "source": [
    "### Solution\n",
    "_pandas_ can cope with columns names being different orders; SQL must have the same order of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What happens if we try to concatenate DataFrames in which the DataFrames only partially share columns (that is, there are some columns in one DataFrame that are not in the other)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a sample DataFrame that contains only a subset of the \n",
    "# columns from an original DataFrame.\n",
    "sample4_df=bldg_2009_10_df[['Reference area','All','Housing-Associations']][:3]\n",
    "sample4_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate two DataFrames with different numbers of columns.\n",
    "concat_diffcolumns_df = pd.concat([sample1_df, sample4_df])\n",
    "concat_diffcolumns_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `concat()` function aligns columns where it can. By default, the columns in the combined DataFrame are the superset of distinctly named columns in the concatenated DataFrame. Missing values are given a NaN value.  \n",
    "\n",
    "This form of concatenation is a type of *outer join* in the sense that we are producing a set of columns in the output that represent the combination of columns contained in the concatenated datasets - the widest possible table - putting empty cells in rows where the original table did not have that column.\n",
    "\n",
    "The `concat()` function uses the outer style join by default as this doesn't lose any data.\n",
    "\n",
    "We can also force it to adopt an *inner join* behaviour in which the columns in the output DataFrame correspond to the intersection of columns from the DataFrames, that is the common columns from the original tables.  Note that the inner join loses data in columns from at least one of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Explicitly use an INNER join ('inner') on the concatenation; 'outer' is the default value.\n",
    "concat_inner_df = pd.concat([sample1_df, sample4_df], join='inner')\n",
    "concat_inner_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that only the common columns appear in the result: all the other data has been lost from the resulting DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns",
    "collapsed": true
   },
   "source": [
    "## Exercise\n",
    "What problems, if any, can you see in interpreting the data in any of the concatenated datasets produced above, and how might they be resolved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns",
    "collapsed": true
   },
   "source": [
    "## Discussion\n",
    "Although the data items represent reports from different years, we have lost that information. \n",
    "\n",
    "The years the reports refer to are not encoded in the actual rows of data - but as *metadata* in the initial rows of the CSV file, and embedded in the filenames.  So, in the concatenated file we have the problem of determining which rows relate to which years.\n",
    "\n",
    "If we add an additional column to each dataset as it is loaded in that contains the year the report relates to, we can carry that information in to the concatenated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns",
    "collapsed": true
   },
   "source": [
    "### Adding in the 'which years' metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity",
    "collapsed": true
   },
   "source": [
    "So how can we add in an additional data column that identifies the period the data relates to before we concatenate the separate DataFrames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR ATTEMPT HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "code_folding": [
     0
    ],
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "# Here's how I did it: \n",
    "bldg_2009_10_df['Period']=\"2009-10\"\n",
    "bldg_2009_10_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Now add a Period column to each annual building DataFrame you created earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# And create a single DataFrame containing all the house building data \n",
    "# with rows distinguishable by period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "source": [
    "## Horizontally joining data: merging data from several datasets\n",
    "\n",
    "By inspection of the building start data and the lettings data, we see that data elements have some common columns: `geographical reference area codes`, and `names`.   \n",
    "\n",
    "The common columns allow us to join rows of data where the values in the common columns are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bldg_2012_13_df = pd.read_csv('data/housingdata/house-building-starts-tenure-2012-2013.csv', \n",
    "                              skiprows=5)\n",
    "bldgSample_df = bldg_2012_13_df[:3]\n",
    "bldgSample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "lettings_2012_13_df = pd.read_csv('data/housingdata/households-social-lettings-general-needs-rents-prp-number-bedrooms-2012-2013.csv',\n",
    "                                  skiprows=5)\n",
    "lettingsSample_df=lettings_2012_13_df[:3]\n",
    "lettingsSample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is straightforward to merge the tables horizontally using the _pandas_ `merge()` function. The first two arguments specify the data tables to be merged. Where the columns that act as the focus for merging share the same name, we can specify them in a list assigned to the `on` parameter.\n",
    "\n",
    "*If you worked through the SQL examples earlier you'll see a similarity to the JOIN ON clause.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "simplemerge_df = pd.merge(bldgSample_df, lettingsSample_df,\n",
    "                          on=['http://opendatacommunities.org/def/ontology/geography/refArea',\n",
    "                              'Reference area'])\n",
    "simplemerge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that we could have also have merged the DataFrames on a single column. In this case, duplicate columns are brought in to the merged result separately, and _pandas_ automatically appends a suffix to each one so it remains uniquely labelled in the resulting DataFrame (so for example we get `Reference area_x` and `Reference area_y` in the result).  Note: *again comparable to the SQL behaviour.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(bldgSample_df, lettingsSample_df,\n",
    "         on=['http://opendatacommunities.org/def/ontology/geography/refArea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This time we have only a single key, with five uniquely named columns from the left table and six from the right.\n",
    "\n",
    "If the column names are differently labelled, we can specify them explicitly for each data table.\n",
    "We can change one of the `lettingsSample_df` column names to demonstrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Renaming one of the merge columns in one table:\n",
    "lettingsSample_df.columns = ['Ref Area Code'] + lettingsSample_df.columns[1:].tolist()\n",
    "lettingsSample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can explicitly declare the columns we want to merge from each table using the `left_on` and `right_on` parameters (I find this confusing, and would have expected `on_left` and `on_right`). \n",
    "\n",
    "For the `merge()` to work, these parameters need to identify the same number of columns in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(bldgSample_df, lettingsSample_df, \n",
    "         left_on=['http://opendatacommunities.org/def/ontology/geography/refArea','Reference area'],\n",
    "         right_on=['Ref Area Code','Reference area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Inner joins:  the merge() default behaviour\n",
    "The default behaviour of *pandas* merge is an inner join (`how='inner'`) where the results table is formed from the intersection of the joined key column values. \n",
    "\n",
    "Consider the example where one table has additional rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bldgSample_long_df = bldg_2012_13_df[:4] # 4 rows compared to 3 in the lettings sample.\n",
    "bldgSample_long_df.columns = ['Ref Area Code'] + bldgSample_long_df.columns[1:].tolist()\n",
    "\n",
    "pd.merge(bldgSample_long_df, lettingsSample_df, \n",
    "         on=['Ref Area Code','Reference area'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the inner join is effectively losing data: where there is no match between the key columns in the two data tables, no row is put into the resulting DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outer joins retain rows from one, or both, of the original DataFrames even if there is no matching row from the 'other' table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Left outer join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In a left outer join we use all the columns from the left table, and matched ones from the right table.\n",
    "\n",
    "Let's generate a long sample from the lettings data but include some different reference areas compared to the building start data. To do this we will take data from the top and the bottom of the original DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "lettingsSample_long_df = pd.concat([lettings_2012_13_df[:2], lettings_2012_13_df[-2:]])\n",
    "lettingsSample_long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Remind yourself of the behaviour of inner joins when there are unmatched rows.\n",
    "# What happens if you try to inner join bldgSample_df and lettingsSample_long_df?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now try a left outer join, by setting `how='left'`. What happens to the columns from the right-hand table for the unmatched rows from the left table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(bldgSample_df, lettingsSample_long_df,\n",
    "         on=['http://opendatacommunities.org/def/ontology/geography/refArea','Reference area'],\n",
    "         how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we see the two key columns, four unique columns from the left table and five unique columns from the right. \n",
    "\n",
    "The final row shows missing values in the right table's columns: it's retained the data from the unmatched rows in the original left table. \n",
    "\n",
    "But we still don't have all the data from both tables - maybe there were unmatched key values in the right table too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Right outer join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Unsurprisingly, a right join is achieved by setting `how='right'`. What happens to the columns from the left-hand table for the unmatched rows from the right column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(bldgSample_df, lettingsSample_long_df,\n",
    "         on=['http://opendatacommunities.org/def/ontology/geography/refArea','Reference area'],\n",
    "         how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Full outer join\n",
    "A full outer join, retaining unmatched rows from both tables, can be achieved by setting `how='outer'`. \n",
    "\n",
    "What happens to the unmatched rows from each table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pd.merge(bldgSample_df, lettingsSample_long_df,\n",
    "         on=['http://opendatacommunities.org/def/ontology/geography/refArea','Reference area'],\n",
    "         how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity",
    "collapsed": true
   },
   "source": [
    "# What happens if a key in one table matches key values in several rows in the second table? \n",
    "\n",
    "*(Note: if you know about relationship modelling, this represents a one-to-many relationship.)*\n",
    "\n",
    "Let's generate a sample DataFrame that has several rows containing the same (repeated) reference area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Two rows from each of two building DataFrames - to create a DataFrame in which \n",
    "# rows have duplicate values for Reference area.  \n",
    "bldg_sample_mixed_df = pd.concat([ bldg_2009_10_df[:2], bldg_2012_13_df[:2] ])\n",
    "bldg_sample_mixed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "### Exercise\n",
    "We can then explore what happens when we try to merge a DataFrame with one unique reference area per row with a DataFrame where there may be multiple rows with repeated values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns",
    "collapsed": true
   },
   "source": [
    "What happens for the various joins (inner, left, right, outer) when applied to `bldg_sample_mixed_df` (which has two rows for Hartlepool and two for Middlesborough) and `lettingsSample_long_df` (which has one row each for Hartlepool, Middlesborough, Wandsworth and Westminster)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# What happens with the inner join on bldg_sample_mixed_df and lettingsSample_long_df?\n",
    "pd.merge(bldg_sample_mixed_df, lettingsSample_long_df,\n",
    "         on=['http://opendatacommunities.org/def/ontology/geography/refArea']  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "#### Discussion\n",
    "If there are repeated matches of one row from one table to multiple rows of another then each joined row is added, leading to repetition of the values from the `on`-row side.   For the inner join any unmatched rows are lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# What happens to the left join on bldg_sample_mixed_df and lettingsSample_long_df?\n",
    "pd.merge(bldg_sample_mixed_df, lettingsSample_long_df,\n",
    "         on=['http://opendatacommunities.org/def/ontology/geography/refArea'],\n",
    "         how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "#### Discussion \n",
    "This is the left outer join, so no rows are lost from the first (left-most) table (`bldg_sample_mixed_df`), and any rows that match repeatedly to rows in the second table are repeated in the result. The usual NaNs fill the unmatched row values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# What happens to the right outer join on bldg_sample_mixed_df and lettingsSample_long_df?\n",
    "pd.merge(bldg_sample_mixed_df, lettingsSample_long_df,\n",
    "         on=['http://opendatacommunities.org/def/ontology/geography/refArea'],\n",
    "         how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "#### Discussion\n",
    "This is the right outer join, so no rows are lost from the second (right-most) table (`lettingsSample_long_df`), and any rows that match repeatedly to rows in the second table are repeated in the result. The usual NaNs fill the unmatched row values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# What happens to the full outer join on bldg_sample_mixed_df and lettingsSample_long_df?\n",
    "pd.merge(bldg_sample_mixed_df, lettingsSample_long_df,\n",
    "         on=['http://opendatacommunities.org/def/ontology/geography/refArea','Reference area'],\n",
    "         how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "#### Discussion\n",
    "This is the full outer join, so no rows are lost from either table, and any rows that match repeatedly to rows in the second table are repeated in the result. The usual NaNs fill the unmatched row values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Merging data tables where one key column represents a unique part of another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Where a scheme using common column identifiers is used to identify the same element or entity that is represented in several datasets, it is easy enough to merge the datasets using the column that contains the common identifier values. \n",
    "\n",
    "In the above examples, we were able to merge data about housing build starts and letting prices across UK administrative areas using the reference area names and/or codes - which the two datasets had in common.\n",
    "\n",
    "In some cases, usually where the datasets have been generated by different organisations or with different data models, the values of the identifiers used in one dataset may only partially match the identifiers in another.  \n",
    "\n",
    "Sometimes, it is possible for us to recreate the identifiers used in one scheme from the identifiers used in another. For example, if one dataset had given a reference area code in an abbreviated form, such as E06000001, we could generate the full identifier from this http://statistics.data.gov.uk/id/statistical-geography/E06000001. \n",
    "This is because the full identifier has a regular pattern http://statistics.data.gov.uk/id/statistical-geography/AREACODE; so given an AREACODE we can recreate the identifier.\n",
    "\n",
    "At other times, the partial match may be more problematic. For example, is 'Open Uni' the same as 'Open University'? Such issues are more in the nature of cleansing and harmonisation issues. More involved data cleansing and harmonisation processes are required to cope with such considerations, which we will ignore for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Well-behaved partial matches\n",
    "The data file `housingdata/yorksAndHumberside.csv` contains a list of administrative areas in the Yorkshire and Humberside adminstrative area. There are three columns in the dataset, taking the form http://data.ordnancesurvey.co.uk/id/7000000000022028, NorthYorkshire, E10000023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv('data/housingdata/yorksAndHumberside.csv')[0:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These contrast with the way administrative areas are recorded in the DCLG datasets, which take the form of two columns e.g. http://statistics.data.gov.uk/id/statistical-geography/E06000002 and `E06000002 Middlesbrough`.  (And of course the DCLG datasets contain data from all over the country, not just Yorskhire and Humberside.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.read_csv('data/housingdata/house-building-starts-tenure-2009-2010.csv', skiprows=5)[0:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activity",
    "collapsed": true
   },
   "source": [
    "## Exercise\n",
    "Looking at the two datasets, the `Reference Area` of the DCLG data looks to be formed by joining the `gss` and `districtname` values into a single string. However, if you look at index row 11 of the Ordnance Survey data and index row 11 of the DCLG data then you'll see that the spacing between district names elements differ.  Closer inspection suggests that the `gss` values are unique for each district, and are properly formed in the DCLG data.\n",
    "\n",
    "So, if you wanted to join these datasets there is some harmonisation required first.\n",
    "\n",
    "Describe how you would adjust the DCLG data so that you could create a DataFrame which could be joined with the Ordnance Survey dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns"
   },
   "source": [
    "## Discussion\n",
    "This requires the `Reference area` column values to be split after the first space.  The string up to the first space should be copied into a new column - this string is the `gss` value, which can be matched to the Ordnance Survey `gss` values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "activity": "activityAns",
    "collapsed": true
   },
   "source": [
    "## Exercise\n",
    "\n",
    "(a) Read in the 2012-13 housing data from the DGLC dataset into a DataFrame, then split the `gss` code values into a new column.\n",
    "\n",
    "(b) Join the housing data with the Ordnance Survey data on the `gss` column, so that the result is the data for Yorkshire and Humberside only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Your solution (a).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Your solution (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "code_folding": [
     0
    ],
    "collapsed": false,
    "run_control": {
     "read_only": true
    }
   },
   "outputs": [],
   "source": [
    "# Solution (a)\n",
    "# Break this down with each step in a seperate cell if you want to see what the intermediate \n",
    "#  results are like.\n",
    "\n",
    "# First bring in the 12-13 dataset.\n",
    "housing1213_df = pd.read_csv('data/housingdata/house-building-starts-tenure-2012-2013.csv', \n",
    "                             skiprows=5)\n",
    "\n",
    "# Now extract the first part of the Reference area column values into a new list.\n",
    "# We saw how to do this when cleaning data, but this time we only need the first column named [0].\n",
    "columnssplitter = lambda x: pd.Series([i for i in (x.split(' '))])\n",
    "\n",
    "split_gss = housing1213_df['Reference area'].apply(columnssplitter)[0]\n",
    "\n",
    "# Add this list to the housing1213_df DataFrame with the column name gss.\n",
    "housing1213_df['gss'] = split_gss\n",
    "\n",
    "# A quick check to see that the added column looks correct.\n",
    "housing1213_df[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "activity": "activity",
    "code_folding": [
     0
    ],
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Solution (b)\n",
    "# Unusually here we actually want to lose data; \n",
    "# we want to lose any rows that don't match in the Ordnance Survey data, \n",
    "# and those in the Ordnance Survey data that don't match in the housing data.\n",
    "# So we want an inner join, and we want to retain those rows that have matching gss values.\n",
    "\n",
    "# First read in the Ordnance Survey data\n",
    "OSdata_df = pd.read_csv('data/housingdata/yorksAndHumberside.csv')\n",
    "\n",
    "# Then merge the two datasets on the gss columns.\n",
    "combineddata_df = pd.merge(housing1213_df, OSdata_df, on=['gss'])\n",
    "\n",
    "# If I were tidying this table up for use later I'd probably lose the \n",
    "# refArea and Reference area columns now, and possibly the district column as well. \n",
    "# (Of course, that depends on what I was using the dataset for, and what I \n",
    "# intended doing with it later.)\n",
    "YorkshireHumbersideHousing201313_df = combineddata_df[['gss', \n",
    "                                                       'districtname',\n",
    "                                                       'All', \n",
    "                                                       'Housing-Associations',\n",
    "                                                       'Local-Authority', \n",
    "                                                       'Private-Enterprise' ]]\n",
    "YorkshireHumbersideHousing201313_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# _pandas_  joins summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "DataFrames can be joined vertically in *pandas* using the `concat()` function, which implements the notion of the `inner` and `outer` union for non-union compatible DataFrames. (This permits tables that don't have the same number of columns to be unioned.)\n",
    "\n",
    "Horizontal joins are achieved using `merge()`. *pandas* merge supports `inner` and `outer`, `full`, `left` and `right` joins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this Notebook, you have seen examples of a number of technqiues for combining data from several tabular datasets.  Extending and enhancing a dataset with data from other datasets is a common requirement - the building block of complex analysis.\n",
    "\n",
    "Once again you will benefit from a build up of case knowledge and experience. Feel free to add to this Notebook as you come up with your own techniques for joining datasets.\n",
    "\n",
    "If you are working through this Notebook as part of an inline exercise, return to the module materials now. If you are working through this set of Notebooks as a whole, move on to `03.4 Handling missing data`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
