{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other data file formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, you will learn how to work with a variety of other file formats. Details for some file formats are left deliberately sparse. If you find yourself spending a lot of time working with such file formats, feel free to add additional notes to this Notebook, or create a new Notebook to record the recipes you find useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spreadsheet files (Excel XLS and XLSX files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although spreadsheet files are one of the most widely used file formats for sharing data, we have relegated them to this Notebook because we want you to get into the habit of using other file formats to publish and request data yourself.  \n",
    "\n",
    "Part 7 of the module looks at some of the weaknesses for analysis and management of data in spreadsheet form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one of the most widely used spreadsheet applications, the file formats used by Microsoft Excel by default are the ones most commonly encountered. Excel spreadsheet files can be recognised from the file extensions *.xls* and *.xlsx*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open a file from a spreadsheet into a *pandas* DataFrame using the `read_excel()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# The xlrd library allows us to read and write files using Excel's .xls and .xlsx formats.\n",
    "\n",
    "import xlrd\n",
    "\n",
    "# The following spreadsheet is taken from the Greater London Authority, London DataStore.\n",
    "#                     https://londondatastore-upload.s3.amazonaws.com/tfl-buses-type.xls\n",
    "#                     [retrieved 20/07/15]\n",
    "workbook = xlrd.open_workbook('data/tfl-buses-type.xls')\n",
    "# The library also allows us to preview the sheet names.\n",
    "print(workbook.sheet_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# We can try to import a sheet directly into pandas using the read_excel() method.\n",
    "# We'll only read the first three lines to see what it brings in.\n",
    "import pandas as pd\n",
    "pd.read_excel('data/tfl-buses-type.xls', sheetname='Data')[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# It looks OK, so let's read the whole spreadsheet:\n",
    "pd.read_excel('data/tfl-buses-type.xls', sheetname='Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting this data, or by opening the spreadsheet using a spreadsheet application or the OpenRefine tool (which is introduced in Part 2 of the module), we can check to see how many of the first few rows are metadata or blank rows. We can discount a certain number of lines at the top of the sheet using the `skiprows` parameter, or we can specify the spreadsheet row number of the header row explictly and ignore the rows preceding that one. We can also define which columns we wish to import.  \n",
    "\n",
    "The NaNs sometimes indicate that cells are empty, or contain formula or other 'non' value data. In the cells under those containing 'Single deck' and 'Double deck' and alongside the description in the final row, the NaNs are there because the cells have been merged into a single spreadsheet spanning cell.\n",
    "\n",
    "(For more information, see the documentation for the [*pandas* read_excel method]( http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_excel.html)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# By manual inspection of the originally previewed sheet, we can use \n",
    "# xlrd to read the metadata from the metadata cell.\n",
    "# Note that row/columns indices are integer values, indexed on 0, \n",
    "# and also note that some cells span multiple rows.\n",
    "sheet = workbook.sheet_by_name('Data')\n",
    "sheet.cell_value(rowx=14, colx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing XML data into a *pandas* DataFrame is currently a little trickier than importing JSON, as there are no default *pandas* methods for supporting the import.\n",
    "\n",
    "Instead, you need to load in a file, parse it using a third party parser such as `lxml`, and then handle the mapping to the DataFrame yourself.\n",
    "\n",
    "Alternatively, use OpenRefine to parse the elements of the XML document that you are interested in and then save the data out again as a tabular CSV document which is a little easier to import."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to limit our use of XML-based datasets in this module, preferring instead CSV formats for tabular data and JSON for more elaborately structured datasets. You will, however, work with a particular style of XML later in the module when you look at Linked Data and the semantic web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing worth bearing in mind is that popular versions of XML formats may have Python libraries defined to make it easier to parse them, and read and write files defined using the format. For example, the KML format that is used to transport geographical data (points, lines, boundaries) can be parsed using the `fastkml` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Working with KML Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We're going to use the fastkml library, which we need to bring in from and exernal source.\n",
    "# Run this when connected to the internet - it will report requirement already satisfied if the\n",
    "# library ia already present, otherwise it will download the fastkml package. \n",
    "!sudo pip3 install fastkml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# We can load in data from a KML file (a file format for geographic data sets) and \n",
    "# then render it onto a map quite easily.\n",
    "\n",
    "# For example, in the data directory is a file, 'CarParks.kml' that contains a list of car park \n",
    "# locations on the Isle of Wight.\n",
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from fastkml import kml\n",
    "k = kml.KML()\n",
    "\n",
    "# We need to open the file as a bytestream - and let the lxml parser \n",
    "#          used by the fastxml package identify the encoding itself:\n",
    "doc = open(\"data/CarParks.kml\",'rb').read()\n",
    "k.from_string(doc)\n",
    "\n",
    "# The alternative is to open the file with a UTF-8 encoding to get a Unicode string, \n",
    "#   then throw away the first line that now incorrectly declares the decoding to be UTF-8.\n",
    "#!head -n 3 data/CarParks.kml\n",
    "#doc = open(\"data/CarParks.kml\", encoding='utf-8')\n",
    "#lines = '\\n'.join(doc.readlines()[1:])\n",
    "#k.from_string(lines)\n",
    "\n",
    "# We can parse the locations of the carpark placemarks from the file\n",
    "locations = dict()\n",
    "for feature in k.features():\n",
    "    for placemark in feature.features():\n",
    "        locations.update({placemark.name: (placemark.geometry.y, placemark.geometry.x)})\n",
    "list(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's quickly map the markers to show how the parser \n",
    "#       has pulled out the placemark information:\n",
    "import folium\n",
    "# We will look at folium in more detail in Notebooks for Part 5 of the module.\n",
    "\n",
    "# NOTE: folium uses an external tileset to render the map background appearance.\n",
    "#       This requires that you have an internet connection when the map is being\n",
    "#       displayed, it may use cached tile data, but some tiles will be missing if you \n",
    "#       change scale by zooming.\n",
    "\n",
    "# If we know the latitude and longitude at the centre of the map we want to display, \n",
    "#    we can set it directly:\n",
    "carparks = folium.Map(location=[50.68, -1.2667], width = 960, height = 500, zoom_start=11)\n",
    "\n",
    "# Alternatively, we could calculate it as the mean latitude and longitude \n",
    "#     of the points we wish to plot (a handy recipe):\n",
    "#latSum = lonSum = 0\n",
    "#for name, location in locations.items():\n",
    "#    latSum += location[0]\n",
    "#    lonSum += location[1]\n",
    "#carparks = folium.Map(location=[latSum/len(locations.items()), \n",
    "#                                lonSum/len(locations.items())], \n",
    "#                               width = 960, height = 500, zoom_start=11)\n",
    "\n",
    "# The following loops through the location items, splitting out the car part name\n",
    "#                    and the location as a pair of latitude and longitude values.\n",
    "# For each location, it then plots a circle marker on the map with the name as a popup string.\n",
    "# We will look at folium in more detail in Notebooks for Part 5 of the module.\n",
    "\n",
    "for name, location in locations.items():\n",
    "    carparks.circle_marker(location=location, popup=name, radius=20,\n",
    "                           line_color='blue', fill_color='blue', fill_opacity=0.2)\n",
    "\n",
    "# Finally we create the HTML file for the map, and display it below.\n",
    "#   (The HTML file can be opened directly from your browser)\n",
    "# This will not display a map if your are offline. \n",
    "carparks.create_map(path = 'data/IOWcarparlocations.html')\n",
    "\n",
    "carparks.render_iframe = True\n",
    "carparks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pandas* does not support YAML imports directly, but it is possible to use libraries such as the `PyYaml` library to load in a YAML file and convert it to a Python dict that can then be transformed to a *pandas* DataFrame.\n",
    "\n",
    "WARNING:  The `yaml.load()` and `yaml.load_all()` should not be used to parse arbitrary content from unsafe sources.  These functions are capable of creating arbitrary Python objects, including code.  The `yaml.safe_load()` and `yaml.safe_load_all()` limit that ability to objects that cannot generate executable code.\n",
    "\n",
    "As with XML, we will tend *not* to focus on the use of YAML, preferring instead JSON and CSV representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# yaml.load() will accept a single document string, and parse it to generate\n",
    "# a Python dict, so will yaml.safe_load():\n",
    "document = \"\"\"\n",
    "image:\n",
    "    width: 800\n",
    "    height: 600\n",
    "    title:  View from 15th Floor\n",
    "    thumbnail:\n",
    "        url: http://www.example.com/image/481989943\n",
    "        height: 125\n",
    "        width:  100\n",
    "        animated : false\n",
    "    IDs:\n",
    "        - 116\n",
    "        - 943\n",
    "        - 234\n",
    "        - 38793\n",
    "\"\"\"\n",
    "parsedYAML = yaml.safe_load(document)\n",
    "parsedYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# yaml.load() and yaml.safe_load() will also accept a file name and read that, \n",
    "#     converting it to a Python dict.\n",
    "\n",
    "# Note that yaml.load_all(stream) and yaml.safe_load_all() will parse a file \n",
    "#  containing a sequence of yaml documents to produce a sequence of dicts\n",
    "\n",
    "# Here 'document.yaml' contains a single YAML document.\n",
    "stream = open('data/document.yaml', 'r') \n",
    "yaml.safe_load(stream)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# We can also cast a dict to YAML using the yaml.dump() function applied to a dict:\n",
    "print(yaml.dump(parsedYAML))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those interested in exploring Python's handling of YAML further\n",
    "the `PyYAML` library documentation can be found at  http://pyyaml.org/wiki/PyYAMLDocumentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this Notebook you have seen how to:\n",
    "1. read .xls and .xlsx spreadsheet files\n",
    "2. handle XML files\n",
    "3. read KML files and seen map data plotted in folium\n",
    "4. parse YAML data and load it into a Python dict.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That completes the coverage of data file formats for this module; we will make extensive use of CSV and JSON formats in the module and may introduce others as we work through different tools and techniques.\n",
    "\n",
    "Return to the module materials now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
